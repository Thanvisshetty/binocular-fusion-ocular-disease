{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1512919,"sourceType":"datasetVersion","datasetId":611716}],"dockerImageVersionId":30396,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=#264653><u>Ocular Disease Recognition</u></font>\n\n## <font color=#264653><u>Introduction</u></font>\nThe eye's fundus is the eye's interior surface opposite the lens, and includes the retina, optic disc, macula, fovea, and posterior pole<sup>[[1]](https://en.wikipedia.org/wiki/Fundus_(eye))</sup>. \n\n<figure>\n<img src=\"https://cdn.3d4medical.com/media/blog/funduscopy/retina-correlation.jpg\" alt=\"fundus anatomy\" width=\"500\" height=\"500\">\n\n<figurecaption><i>Image from [3d4medical](https://3d4medical.com/blog/funduscopy)</i></figurecaption>\n</figure>\n\nOphthalmologists use fundus photography to detect fundus disease such as diabetic retinopathy, glaucoma, age-related macular degeneration, cataracts, hypertension, and myopia. Computer-aided diagnosis have been progressively adopted by ophthalmologists, as its accuracy increased in recent years<sup>[[2]](https://pesquisa.bvsalud.org/portal/resource/pt/wpr-822979)</sup>.\n\n## <font color=#264653><u>Data</u></font>\nThe data in this notebook is the ODIR binocular fundus image dataset [[3]](https://pesquisa.bvsalud.org/portal/resource/pt/wpr-822979), containing color fundus photographs (CFP), available as multi-class multi-label instances. As we will further discuss while exploring the dataset, there are several different ML problems which can be derived from this dataset - in this notebook, I chose to tackle a binary classification problem, only focusing on a single ocular disease.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color=#264653><u>Imports</u></font>","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport pandas as pd\npd.options.mode.chained_assignment = None;\nimport numpy as np\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.manifold import TSNE\n\nfrom tensorflow.keras.applications import MobileNetV3Small\n\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\n\nimport random\nimport re\n\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\ndef seed_it_all(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_it_all()\n\nprint(f\"\\n...COMPLETED IMPORT...\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:34.478796Z","iopub.execute_input":"2023-03-12T15:41:34.479226Z","iopub.status.idle":"2023-03-12T15:41:41.30117Z","shell.execute_reply.started":"2023-03-12T15:41:34.479189Z","shell.execute_reply":"2023-03-12T15:41:41.30009Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color=#264653><u>Accelerator Setup</u></font>","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()} ...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    physical_devices = tf.config.list_physical_devices('GPU')\n    try:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    except:\n        # Invalid device or cannot modify virtual devices once initialized.\n        pass\n    strategy = tf.distribute.get_strategy()\n\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.303309Z","iopub.execute_input":"2023-03-12T15:41:41.304251Z","iopub.status.idle":"2023-03-12T15:41:41.50668Z","shell.execute_reply.started":"2023-03-12T15:41:41.304212Z","shell.execute_reply":"2023-03-12T15:41:41.505612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color=#264653><u>Data Loading</u></font>","metadata":{}},{"cell_type":"code","source":"DATA_PATH_INIT = \"ocular-disease-recognition-odir5k\"\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path(DATA_PATH_INIT)\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/\" + DATA_PATH_INIT\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.50912Z","iopub.execute_input":"2023-03-12T15:41:41.509479Z","iopub.status.idle":"2023-03-12T15:41:41.537891Z","shell.execute_reply.started":"2023-03-12T15:41:41.509445Z","shell.execute_reply":"2023-03-12T15:41:41.53682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(\"/kaggle/input\", DATA_PATH_INIT, \"full_df.csv\"))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.540363Z","iopub.execute_input":"2023-03-12T15:41:41.540796Z","iopub.status.idle":"2023-03-12T15:41:41.613862Z","shell.execute_reply.started":"2023-03-12T15:41:41.540761Z","shell.execute_reply":"2023-03-12T15:41:41.612907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.615286Z","iopub.execute_input":"2023-03-12T15:41:41.61568Z","iopub.status.idle":"2023-03-12T15:41:41.643Z","shell.execute_reply.started":"2023-03-12T15:41:41.615631Z","shell.execute_reply":"2023-03-12T15:41:41.642083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color=#264653><u>Constants and Helper Functions</u></font>","metadata":{}},{"cell_type":"code","source":"labels_long = [\"Normal\", \"Diabetes\", \"Glaucoma\", \"Cataract\", \"AMD\", \"Hypertension\", \"Myopia\", \"Other\"]\nlabels_short = [ll[0] for ll in labels_long]\n\nclass_short2full = {\n    ls: ll\n    for ls, ll in zip(labels_short, labels_long)\n}\n\nclass_dict = {class_ : i for i, class_ in enumerate(class_short2full.keys())}\nclass_dict_rev = {v: k for k, v in class_dict.items()}\n\nNUM_CLASSES = len(class_dict)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.644456Z","iopub.execute_input":"2023-03-12T15:41:41.64487Z","iopub.status.idle":"2023-03-12T15:41:41.651924Z","shell.execute_reply.started":"2023-03-12T15:41:41.64483Z","shell.execute_reply":"2023-03-12T15:41:41.650679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 42\n\nCOLORS = {\n    \"fig_bg\": \"#f6f5f5\",\n    \"plot_neut\": \"#ddbea9\",\n    \"plot_text\": \"#343a40\",\n    \n    \"cmap_color_list\": [\"#001219\", \"#005F73\", \"#0A9396\", \"#94D2BD\", \"#E9D8A6\",\n                        \"#EE9B00\", \"#CA6702\", \"#BB3E03\", \"#AE2012\", \"#9B2226\"],\n    \n    \"split\": {\n        \"train\": \"#264653\",\n        \"val\": \"#2a9d8f\",\n        \"test\": \"#e9c46a\"\n    }\n}\n\nCOLORS[\"class\"] = {ls: c for ls, c in zip(class_short2full.keys(), COLORS[\"cmap_color_list\"][:len(class_short2full.keys())])}\nCOLORS[\"cmap\"] = mpl.colors.LinearSegmentedColormap.from_list(\"\", COLORS[\"cmap_color_list\"])\nCOLORS[\"cmap_pos\"] = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"#F0F3F8\", \"#D1DBE9\", \"#A2B7D2\", \"#7493BC\", \"#6487B4\", \"#3D5A80\"])\n\ncolors_class_list = list(COLORS[\"class\"].values())\n\nFONT_KW = {\n    \"plot_title\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"25\",\n        \"style\": \"normal\"\n    },\n    \"plot_title_small\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"16\",\n        \"style\": \"normal\"\n    },\n    \"plot_subtitle\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"12\",\n        \"style\": \"normal\"\n    },\n    \"subplot_title\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"18\",\n        \"style\": \"normal\"\n    },\n    \"subplot_title_small\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"12\",\n        \"style\": \"normal\"\n    },\n    \"plot_label\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"16\",\n        \"style\": \"normal\"\n    },\n    \"plot_label_small\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"bold\",\n        \"size\": \"12\",\n        \"style\": \"normal\"\n    },\n    \"plot_text\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"normal\",\n        \"size\": \"12\",\n        \"style\": \"normal\"\n    },\n    \"plot_text_small\" : {\n        \"fontname\": \"serif\",\n        \"weight\": \"normal\",\n        \"size\": \"8\",\n        \"style\": \"normal\"\n    },\n}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.653739Z","iopub.execute_input":"2023-03-12T15:41:41.654231Z","iopub.status.idle":"2023-03-12T15:41:41.668545Z","shell.execute_reply.started":"2023-03-12T15:41:41.654197Z","shell.execute_reply":"2023-03-12T15:41:41.667372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_values_relative(y):\n    bins, vals = np.unique(y, return_counts=True)\n    return bins, 100 * vals / np.sum(vals)\n\ndef ceil_d(n, d=1000):\n    return int(np.ceil(n / d) * d)\n\ndef get_subplot_dims(N):\n    r = np.ceil(np.sqrt(N))\n    c = np.floor(np.sqrt(N))\n    if r*c < N:\n        r += 1\n    return int(r), int(c)\n\n\n# human sorting based on https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\ndef natural_keys(text):\n    '''\n    alist.sort(key=natural_keys) sorts in human order\n    http://nedbatchelder.com/blog/200712/human_sorting.html\n    '''\n    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n\ndef natural_sort_col_unique(df, colname, missing=\"NA\"):\n    arr = df[colname].unique().tolist()\n    if np.nan in arr:\n        arr[arr.index(np.nan)] = missing\n    arr.sort(key=natural_keys)\n    return arr\n\n    \ndef conditional_entropy(x,y):\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] / total_occurrences\n        p_y = y_counter[xy[1]] / total_occurrences\n        entropy += p_xy * math.log(p_y/p_xy)\n    return entropy\n\ndef theil_u(x,y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) / s_x\n    \ndef correlation_ratio(categories, measurements):\n    if isinstance(categories, pd.Series):\n        categories = categories.values\n    if isinstance(measurements, pd.Series):\n        measurements = measurements.values\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat) + 1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0, cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n    numerator = np.sum(\n        np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg),\n                                      2)))\n    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator / denominator)\n    return eta","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.671965Z","iopub.execute_input":"2023-03-12T15:41:41.672352Z","iopub.status.idle":"2023-03-12T15:41:41.690743Z","shell.execute_reply.started":"2023-03-12T15:41:41.672323Z","shell.execute_reply":"2023-03-12T15:41:41.689583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/ocular-disease-recognition-odir5k/preprocessed_images\"\nIMG_SIZE = 224\nIMAGE_SIZE = [IMG_SIZE, IMG_SIZE]\n\ndef label_image(c):\n    label = np.full((NUM_CLASSES), 0, dtype=int)\n    label[c] = 1\n    return label\n\ndef get_gaussian_filter_shape(IMG_SIZE):\n    return IMG_SIZE//4 - 1\n\ndef blur_image(image, sigma=10):\n    filter_shape=get_gaussian_filter_shape(IMG_SIZE)\n    return tfa.image.gaussian_filter2d(image, filter_shape=filter_shape, sigma=sigma)\n\ndef weighted_image(image, alpha=4, beta=-4, gamma=128):\n    return image*alpha + blur_image(image)*beta + gamma\n\ndef create_dataset(dataset, img_list, class_label, augment={}):\n    for img in img_list:\n        image_path = os.path.join(DATA_PATH, img)\n        image_label = label_image(class_label)\n        try:\n            image = cv2.imread(image_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image,(IMG_SIZE, IMG_SIZE))\n            image = weighted_image(image)\n        except:\n            continue\n        \n        dataset.append([np.array(image), image_label])\n        \n        if augment:\n            if class_label in augment.keys():\n                if augment[class_label]:\n                    image_lr = tf.image.flip_left_right(image)\n                    image_ud = tf.image.flip_up_down(image)\n                    image_rot90 = tf.image.rot90(image, k=1)\n                    image_rot180 = tf.image.rot90(image, k=2)\n                    image_rotm90 = tf.image.rot90(image, k=-1)\n                    \n                    dataset.append([np.array(image_lr), image_label])\n                    dataset.append([np.array(image_ud), image_label])\n                    dataset.append([np.array(image_rot90), image_label])\n                    dataset.append([np.array(image_rot180), image_label])\n                    dataset.append([np.array(image_rotm90), image_label])\n        \n    random.shuffle(dataset)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.692639Z","iopub.execute_input":"2023-03-12T15:41:41.693149Z","iopub.status.idle":"2023-03-12T15:41:41.708894Z","shell.execute_reply.started":"2023-03-12T15:41:41.693108Z","shell.execute_reply":"2023-03-12T15:41:41.707893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color=#264653><u>Exploratory Data Analysis</u></font>","metadata":{}},{"cell_type":"code","source":"df_eda = df.copy()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.713546Z","iopub.execute_input":"2023-03-12T15:41:41.713803Z","iopub.status.idle":"2023-03-12T15:41:41.722149Z","shell.execute_reply.started":"2023-03-12T15:41:41.71378Z","shell.execute_reply":"2023-03-12T15:41:41.721015Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_eda[\"class\"] = df_eda[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.723593Z","iopub.execute_input":"2023-03-12T15:41:41.724273Z","iopub.status.idle":"2023-03-12T15:41:41.743693Z","shell.execute_reply.started":"2023-03-12T15:41:41.724233Z","shell.execute_reply":"2023-03-12T15:41:41.742784Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 6), dpi=70, gridspec_kw={\"wspace\": 0.5})\n\nfig.patch.set_facecolor(COLORS[\"fig_bg\"])\n\nvalue_counts = df_eda[\"class\"].value_counts().rename(\"num\").to_frame()\nvalue_counts[\"percent\"] = value_counts / value_counts.sum()\nvalue_counts.reindex(index=COLORS[\"class\"].keys())\n\nb1 = ax1.barh(value_counts.index, value_counts[\"percent\"])\n\nax1.set_yticks(\n    value_counts.index,\n    [class_short2full[i] for i in value_counts.index],\n    **FONT_KW[\"plot_label\"], color=COLORS[\"plot_text\"]\n)\nax1.tick_params(axis=\"y\", length=0)\nax1.set_title(\"Label-Based\", loc=\"left\", **FONT_KW[\"subplot_title\"], color=COLORS[\"plot_text\"], pad=30)\nax1.text(0, 8.2, \"(Multi-class)\", **FONT_KW[\"subplot_title_small\"], color=COLORS[\"plot_text\"])\n\nax1.bar_label(\n    b1,\n    labels=[str(val) + f\"\\n({str(np.round(100*pcnt,1))}%)\" for val, pcnt in zip(value_counts[\"num\"], value_counts[\"percent\"])],\n    color=COLORS[\"plot_text\"],\n    **FONT_KW[\"plot_text\"]\n)\n\nax1.set_facecolor(COLORS[\"fig_bg\"])\nfor i in range(NUM_CLASSES):\n    c = COLORS[\"class\"][value_counts.index[i]]\n    ax1.get_yticklabels()[i].set_color(c)\n    b1[i].set_color(c)\n\nax1.axes.get_xaxis().set_visible(False)\n\nfor spine in [\"bottom\", \"right\", \"top\"]:\n    ax1.spines[spine].set_visible(False)\n\n\nvalue_count_diag = df_eda[labels_short].sum().rename(\"num\").to_frame()\nvalue_count_diag[\"percent\"] = value_count_diag / df_eda.shape[0]\nvalue_count_diag = value_count_diag.reindex(index=value_counts.index)\n\nb2 = ax2.barh(value_count_diag.index, value_count_diag[\"percent\"])\n\nax2.set_yticks(\n    value_count_diag.index,\n    [class_short2full[i] for i in value_count_diag.index],\n    **FONT_KW[\"plot_label\"], color=COLORS[\"plot_text\"]\n)\nax2.tick_params(axis=\"y\", length=0)\nax2.set_title(\"Diagnosis-Based\", loc=\"left\", **FONT_KW[\"subplot_title\"], color=COLORS[\"plot_text\"], pad=30)\nax2.text(0, 8.2, \"(Multi-class Multi-label)\", **FONT_KW[\"subplot_title_small\"], color=COLORS[\"plot_text\"])\n\nax2.bar_label(\n    b2,\n    labels=[str(val) + f\"\\n({str(np.round(100*pcnt,1))}%)\" for val, pcnt in zip(value_count_diag[\"num\"], value_count_diag[\"percent\"])],\n    color=COLORS[\"plot_text\"],\n    **FONT_KW[\"plot_text\"]\n)\n\nax2.set_facecolor(COLORS[\"fig_bg\"])\nfor i in range(NUM_CLASSES):\n    c = COLORS[\"class\"][value_count_diag.index[i]]\n    ax2.get_yticklabels()[i].set_color(c)\n    b2[i].set_color(c)\n\nax2.axes.get_xaxis().set_visible(False)\n\nfor spine in [\"bottom\", \"right\", \"top\"]:\n    ax2.spines[spine].set_visible(False)\n    \nplt.figtext(0, 1.05, \"Class Distribution\", **FONT_KW[\"plot_title\"], color=COLORS[\"plot_text\"])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:41.745909Z","iopub.execute_input":"2023-03-12T15:41:41.746395Z","iopub.status.idle":"2023-03-12T15:41:42.123621Z","shell.execute_reply.started":"2023-03-12T15:41:41.746361Z","shell.execute_reply":"2023-03-12T15:41:42.122609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can frame the problem, as presented in the available dataset, in several different ways, e.g.:\n- A <b><u>multi-class multi-label classification</u></b> problem, where each instance (image) can contain several classes (e.g., an image belonging to both <i>Diabetes</i> and <i>Other</i> classes).\n- A <b><u>multi-class classification</u></b> problem, where each instance can belong to a single class (by using the final labels as classes, e.g.).\n- A <b><u>binary classification</u></b> problem, where only instances belonging to 2 classes are samples from the dataset (e.g., only <i>Cataract</i> and <i>Normal</i> instances).\n\nNote that in the above dataframe, each instance contains some information regarding the specific image (e.g., <i>filename</i> and <i>labels</i>) and some information regarding the patient, i.e., each row in the dataframe may contain diagnostic information regarding the right and eft eyes, but the final <i>label</i> only codes the class of the image depicted in the <i>filename</i>.","metadata":{}},{"cell_type":"code","source":"df[\"class\"] = df[\"labels\"].apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:42.125255Z","iopub.execute_input":"2023-03-12T15:41:42.125998Z","iopub.status.idle":"2023-03-12T15:41:42.144117Z","shell.execute_reply.started":"2023-03-12T15:41:42.125958Z","shell.execute_reply":"2023-03-12T15:41:42.143189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dict_img_list = {\n    class_: df.loc[df[\"class\"]==class_][\"filename\"].values\n    for class_ in class_short2full.keys()\n}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:42.145811Z","iopub.execute_input":"2023-03-12T15:41:42.146274Z","iopub.status.idle":"2023-03-12T15:41:42.163069Z","shell.execute_reply.started":"2023-03-12T15:41:42.146238Z","shell.execute_reply":"2023-03-12T15:41:42.162189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will generate a small dataset with a few samples from each class.\nThen, we will use this smaller dataset to visualize the 2D embeddings calculated by t-SNE, to get a sense of which classes or more likely to be similar and which are more easily differentiable:","metadata":{}},{"cell_type":"code","source":"# based on https://github.com/ageron/handson-ml2/blob/master/08_dimensionality_reduction.ipynb\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\ndef plot_classes(X, y, min_distance=0.05, images=None, figsize=(13, 10), cmap=COLORS[\"cmap\"], annot=False):\n    # Let's scale the input features so that they range from 0 to 1\n    X_normalized = MinMaxScaler().fit_transform(X)\n    # Now we create the list of coordinates of the digits plotted so far.\n    # We pretend that one is already plotted far away at the start, to\n    # avoid `if` statements in the loop below\n    neighbors = np.array([[10., 10.]])\n    fig, ax = plt.subplots(figsize=figsize)\n    classes = np.unique(y)\n    n_classes = len(classes)\n    for class_ in classes:\n        ax.scatter(\n            X_normalized[y == class_, 0],\n            X_normalized[y == class_, 1],\n            c=COLORS[\"class\"][class_dict_rev[class_]],\n            alpha=0.7,\n        )\n    \n    if annot:\n        for index, image_coord in enumerate(X_normalized):\n            closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n            if closest_distance > min_distance:\n                neighbors = np.r_[neighbors, [image_coord]]\n                if images is None:\n                    ax.text(\n                        image_coord[0],\n                        image_coord[1],\n                        class_dict_rev[y[index]],\n                        color=COLORS[\"class\"][class_dict_rev[y[index]]],\n                        alpha=0.7,\n                        **FONT_KW[\"plot_text_small\"]\n                    )\n                else:\n                    image = images[index].reshape(28, 28)\n                    imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                    ax.add_artist(imagebox)\n    \n    fig.patch.set_facecolor(COLORS[\"fig_bg\"])\n    ax.set_facecolor(COLORS[\"fig_bg\"])\n    ax.axis(\"off\")\n    \n    ax.legend(\n        [class_short2full[class_dict_rev[label]] for label in np.unique(y)],\n        prop={\"family\": \"serif\", \"size\": 8},\n        facecolor=COLORS[\"fig_bg\"]\n    )\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:42.164597Z","iopub.execute_input":"2023-03-12T15:41:42.16524Z","iopub.status.idle":"2023-03-12T15:41:42.177083Z","shell.execute_reply.started":"2023-03-12T15:41:42.165198Z","shell.execute_reply":"2023-03-12T15:41:42.175963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_images_per_class = 50\nrng = np.random.default_rng(seed=SEED)\n\nfor class_ in class_dict.keys():\n    ind = rng.choice(len(dict_img_list[class_]), n_images_per_class, replace=False)\n    dict_img_list[class_] = dict_img_list[class_][ind]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:42.178781Z","iopub.execute_input":"2023-03-12T15:41:42.179699Z","iopub.status.idle":"2023-03-12T15:41:42.189765Z","shell.execute_reply.started":"2023-03-12T15:41:42.179615Z","shell.execute_reply":"2023-03-12T15:41:42.189085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_CLASSES = len(class_dict)\ndataset_viz = []\nprint(\"START building visualization dataset\")\nfor i, class_ in enumerate(class_dict.keys()):\n    print(f\"[{i+1}/{len(class_dict)}] adding {class_short2full[class_]} to dataset ...\")\n    dataset_viz = create_dataset(dataset_viz, dict_img_list[class_], class_dict[class_])\nprint(\"COMPLETE building visualization dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:42.191424Z","iopub.execute_input":"2023-03-12T15:41:42.192353Z","iopub.status.idle":"2023-03-12T15:41:53.805056Z","shell.execute_reply.started":"2023-03-12T15:41:42.192326Z","shell.execute_reply":"2023-03-12T15:41:53.803931Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_viz = np.array([i[0] for i in dataset_viz]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\ny_viz = np.array([i[1] for i in dataset_viz])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:53.80646Z","iopub.execute_input":"2023-03-12T15:41:53.807291Z","iopub.status.idle":"2023-03-12T15:41:53.834023Z","shell.execute_reply.started":"2023-03-12T15:41:53.807251Z","shell.execute_reply":"2023-03-12T15:41:53.832974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tsne = TSNE(\n    n_components=2,\n    init=\"pca\",\n    learning_rate=\"auto\",\n    perplexity=50,\n    n_iter=5000,\n    random_state=SEED\n)\n  \nX_viz = X_viz.reshape((X_viz.shape[0], np.prod(X_viz.shape[1:])))\nX_tsne_reduced = tsne.fit_transform(X_viz)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:41:53.835618Z","iopub.execute_input":"2023-03-12T15:41:53.835987Z","iopub.status.idle":"2023-03-12T15:42:11.905111Z","shell.execute_reply.started":"2023-03-12T15:41:53.83595Z","shell.execute_reply":"2023-03-12T15:42:11.904203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_classes(X_tsne_reduced, y_viz.argmax(axis=1), figsize=(8,8), annot=True, min_distance=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:11.906658Z","iopub.execute_input":"2023-03-12T15:42:11.907469Z","iopub.status.idle":"2023-03-12T15:42:12.576992Z","shell.execute_reply.started":"2023-03-12T15:42:11.907427Z","shell.execute_reply":"2023-03-12T15:42:12.575967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_viz_label = y_viz.argmax(axis=1)\nind_selected_samples = (y_viz_label == class_dict[\"N\"]) | (y_viz_label == class_dict[\"G\"])\nplot_classes(X_tsne_reduced[ind_selected_samples], y_viz_label[ind_selected_samples], figsize=(4,4), annot=True, min_distance=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:12.578052Z","iopub.execute_input":"2023-03-12T15:42:12.579103Z","iopub.status.idle":"2023-03-12T15:42:12.881198Z","shell.execute_reply.started":"2023-03-12T15:42:12.579062Z","shell.execute_reply":"2023-03-12T15:42:12.880238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also examine the 2D embeddings learned by UMAP:","metadata":{}},{"cell_type":"code","source":"from umap import UMAP\n\nreducer = UMAP()\n\nX_umap_reduced = reducer.fit_transform(X_viz)\nplot_classes(X_umap_reduced, y_viz.argmax(axis=1), figsize=(8,8), annot=True, min_distance=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:12.883086Z","iopub.execute_input":"2023-03-12T15:42:12.883742Z","iopub.status.idle":"2023-03-12T15:42:39.749721Z","shell.execute_reply.started":"2023-03-12T15:42:12.883705Z","shell.execute_reply":"2023-03-12T15:42:39.748736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_classes(X_umap_reduced[ind_selected_samples], y_viz_label[ind_selected_samples], figsize=(4,4), annot=True, min_distance=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:39.751059Z","iopub.execute_input":"2023-03-12T15:42:39.751817Z","iopub.status.idle":"2023-03-12T15:42:40.056526Z","shell.execute_reply.started":"2023-03-12T15:42:39.751775Z","shell.execute_reply":"2023-03-12T15:42:40.055321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As a starting point, we will frame the problem as a <b><u>binary classification</u></b> problem, and will only use instances from the <font color=\"#0A9396\"><b><i>Glaucoma</i></b></font> and <font color=\"#001219\"><b><i>Normal</i></b></font> classes. For this purpose, we will use the final <i>label</i> for each instance, to select the images based on their <i>filename</i>.\n\nNow, we will generate the full dataset from the chosen classes:","metadata":{}},{"cell_type":"markdown","source":"## <font color=#264653><u>Generating Dataset</u></font>","metadata":{}},{"cell_type":"code","source":"CLASSES = [\"N\", \"G\"]\nNUM_CLASSES = len(CLASSES)\n\nclass_dict = {class_ : i for i, class_ in enumerate(CLASSES)}\nclass_dict_rev = {v: k for k, v in class_dict.items()}\n\ndf = df.loc[df[\"class\"].isin(CLASSES)]\n\ndict_img_list = {\n    class_: df.loc[df[\"class\"]==class_][\"filename\"].values\n    for class_ in CLASSES\n}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:40.058174Z","iopub.execute_input":"2023-03-12T15:42:40.058592Z","iopub.status.idle":"2023-03-12T15:42:40.070283Z","shell.execute_reply.started":"2023-03-12T15:42:40.058556Z","shell.execute_reply":"2023-03-12T15:42:40.069328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,6), dpi=70, gridspec_kw={\"wspace\": 0.5})\n\nfig.patch.set_facecolor(COLORS[\"fig_bg\"])\n\nvalue_counts = df[\"class\"].value_counts().rename(\"num\").to_frame()\nvalue_counts[\"percent\"] = value_counts / value_counts.sum()\nvalue_counts.reindex(index=CLASSES)\n\nb1 = ax1.barh(value_counts.index, value_counts[\"percent\"])\n\nax1.set_yticks(\n    value_counts.index,\n    [class_short2full[i] for i in value_counts.index],\n    **FONT_KW[\"plot_label_small\"], color=COLORS[\"plot_text\"]\n)\nax1.tick_params(axis=\"y\", length=0)\nax1.set_title(\"Original\", loc=\"left\", **FONT_KW[\"subplot_title\"], color=COLORS[\"plot_text\"])\n\nax1.bar_label(\n    b1,\n    labels=[str(val) + f\"\\n({str(np.round(100*pcnt,1))}%)\" for val, pcnt in zip(value_counts[\"num\"], value_counts[\"percent\"])],\n    padding=5,\n    color=COLORS[\"plot_text\"],\n    **FONT_KW[\"plot_text\"]\n)\n\nax1.set_facecolor(COLORS[\"fig_bg\"])\n\nfor i in range(NUM_CLASSES):\n    c = COLORS[\"class\"][value_counts.index[i]]\n    ax1.get_yticklabels()[i].set_color(c)\n    b1[i].set_color(c)\n\nax1.axes.get_xaxis().set_visible(False)\n\nfor spine in [\"bottom\", \"right\", \"top\"]:\n    ax1.spines[spine].set_visible(False)\n\n\nNUM_AUGMENTATIONS = 5\nvalue_counts_aug = value_counts.copy()\nvalue_counts_aug.loc[\"G\", \"num\"] *= NUM_AUGMENTATIONS\nvalue_counts_aug[\"percent\"] = value_counts_aug[\"num\"] / value_counts_aug[\"num\"].sum()\n    \nb2 = ax2.barh(value_counts_aug.index, value_counts_aug[\"percent\"])\n\nax2.set_yticks(\n    value_counts_aug.index,\n    [class_short2full[i] for i in value_counts_aug.index],\n    **FONT_KW[\"plot_label_small\"], color=COLORS[\"plot_text\"]\n)\nax2.tick_params(axis=\"y\", length=0)\nax2.set_title(\"With Minority Class Augmentations\", loc=\"left\", **FONT_KW[\"subplot_title\"], color=COLORS[\"plot_text\"])\n\nax2.bar_label(\n    b2,\n    labels=[str(val) + f\"\\n({str(np.round(100*pcnt,1))}%)\" for val, pcnt in zip(value_counts_aug[\"num\"], value_counts_aug[\"percent\"])],\n    padding=5,\n    color=COLORS[\"plot_text\"],\n    **FONT_KW[\"plot_text\"]\n)\n\nax2.set_facecolor(COLORS[\"fig_bg\"])\n\nfor i in range(NUM_CLASSES):\n    c = COLORS[\"class\"][value_counts_aug.index[i]]\n    ax2.get_yticklabels()[i].set_color(c)\n    b2[i].set_color(c)\n\nax2.axes.get_xaxis().set_visible(False)\n\nfor spine in [\"bottom\", \"right\", \"top\"]:\n    ax2.spines[spine].set_visible(False)\n    \nplt.figtext(0, 1.05, \"Class Distribution\", **FONT_KW[\"plot_title\"], color=COLORS[\"plot_text\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:40.072166Z","iopub.execute_input":"2023-03-12T15:42:40.07296Z","iopub.status.idle":"2023-03-12T15:42:40.267903Z","shell.execute_reply.started":"2023-03-12T15:42:40.072922Z","shell.execute_reply":"2023-03-12T15:42:40.266581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When generating the dataset from images of the chosen classes, we will somewhat try to alleviate the class imbalance, as there are predominantly fewer instances of <font color=\"#0A9396\"><b><i>Glaucoma</i></b></font> than <font color=\"#001219\"><b><i>Normal</i></b></font> instances.\n\nOne way to do that is by <b><u>data augmentation</u></b>: when loading images, we will add augmented version of instances from the minority class (i.e., <font color=\"#0A9396\"><b><i>Glaucoma</i></b></font>). The augmentation used in this case are affine image transformations, specifically, flipping and rotating the input image: flipping vertically and horizontally, rotating counter clockwise by 90° and 180°, and rotating clockwise by 90°. In addition to significantly increasing the number of training instances, such augmentations also introduce some positional invariance, as they allow the model to learn different possible locations for the regions of interest (e.g., a lesion, atrophy, etc.). Nonetheless, such traditional data augmentation techniques are often insufficient, as they do not regularize the model enough<sup>[[4]](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1807.10225.pdf)</sup>.\n\nOther image augmentation techniques are possible, such as elastic transformations, pixel-level augmentation (e.g., modifying brightness, sharpening, blurring, and more), generating artificial data (e.g., by using GANs), and more, however, I will not apply those in this notebook.\n\nIn their article <i>\"Multi-Label Fundus Image Classification Using Attention Mechanisms and Feature Fusion\"</i> <sup>[[5]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9230753/)</sup>, Zhenwei et. al. tackle this dataset as a multi-class multi-label problem. They suggest an <i>Image Augmentation Model</i> which performs affine transformations (rotations and flips) on image-weighted enhanced version of the input images, as:\n\n$I_{weight} = I_{org}\\ast\\alpha+I_{blur}\\ast\\beta + \\gamma$\n\n$I_{blur}=I_{org}\\ast{kernel_{h\\times{w}}}$\n\nThat is, the input image is first convolved with a gaussian filter, and the image-weighted enhancement is a linear combination of the original image and the blurred image. In their paper, Zhenwei et. al. used a gaussian kernel with $h=w=63$, however, they resized the input images to have the dimensions $H=W=256$, whereas I opted for slightly smaller images with $H=W=224$; hence, I adjusted the kernel's dimensions to be $h=w=224*0.25-1 = 55$. ","metadata":{}},{"cell_type":"code","source":"augment = {\n    class_dict[\"N\"]: False,\n    class_dict[\"G\"]: True,\n}\n\ndataset = []\nprint(\"START building dataset\")\nfor i, class_ in enumerate(CLASSES):\n    print(f\"[{i+1}/{len(CLASSES)}] adding {class_short2full[class_]} to dataset ...\")\n    dataset = create_dataset(dataset, dict_img_list[class_], class_dict[class_], augment=augment)\nprint(\"COMPLETE building dataset\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:42:40.26976Z","iopub.execute_input":"2023-03-12T15:42:40.270478Z","iopub.status.idle":"2023-03-12T15:43:47.828567Z","shell.execute_reply.started":"2023-03-12T15:42:40.270427Z","shell.execute_reply":"2023-03-12T15:43:47.826574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.array([i[0] for i in dataset]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\ny = np.array([i[1] for i in dataset])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:47.829835Z","iopub.execute_input":"2023-03-12T15:43:47.830218Z","iopub.status.idle":"2023-03-12T15:43:48.047979Z","shell.execute_reply.started":"2023-03-12T15:43:47.830181Z","shell.execute_reply":"2023-03-12T15:43:48.046928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font color=#264653><u>Data Sampling</u></font>\nWe will use 60% of the dataset for training, 20% for validation, and 20% for testing.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.2, stratify=y, random_state=SEED)\nX_train, X_val, y_train, y_val = train_test_split(X_train , y_train, test_size=0.25, stratify=y_train, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:48.054484Z","iopub.execute_input":"2023-03-12T15:43:48.054784Z","iopub.status.idle":"2023-03-12T15:43:48.462539Z","shell.execute_reply.started":"2023-03-12T15:43:48.054756Z","shell.execute_reply":"2023-03-12T15:43:48.461539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can plot some samples from each class to examine the training instances before they are fed into a model:","metadata":{}},{"cell_type":"code","source":"def plot_samples(n_images_per_class=8, figsize=(8, 8), seed=SEED):\n    rng = np.random.default_rng(seed=seed)\n\n    num_images = n_images_per_class*NUM_CLASSES\n    nrows, ncols = get_subplot_dims(num_images)\n\n    idx_neg = np.concatenate(\n        (\n            np.arange(start=1, stop=num_images, step=nrows),\n            np.arange(start=1, stop=num_images, step=nrows)+1\n        )\n    )\n    idx_neg.sort()\n\n    images_neg = X_train[y_train.argmax(axis=1)==0]\n    ind_neg = rng.choice(len(images_neg), n_images_per_class, replace=False)\n\n    images_pos = X_train[y_train.argmax(axis=1)==1]\n    ind_pos = rng.choice(len(images_pos), n_images_per_class, replace=False)\n\n    fig = plt.figure(figsize=figsize, tight_layout=True)\n    count_neg = 0\n    count_pos = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            img_ind = row*ncols + col + 1\n            ax = plt.subplot(nrows, ncols, img_ind)\n            if img_ind in idx_neg:\n                ax.imshow(images_neg[ind_neg[count_neg]])\n                count_neg += 1\n                ax.set_title(class_short2full[class_dict_rev[0]], color=COLORS[\"class\"][class_dict_rev[0]], **FONT_KW[\"subplot_title_small\"])\n            else:\n                ax.imshow(images_pos[ind_pos[count_pos]])\n                count_pos += 1\n                ax.set_title(class_short2full[class_dict_rev[1]], color=COLORS[\"class\"][class_dict_rev[1]], **FONT_KW[\"subplot_title_small\"])\n            ax.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:48.464122Z","iopub.execute_input":"2023-03-12T15:43:48.464536Z","iopub.status.idle":"2023-03-12T15:43:48.476456Z","shell.execute_reply.started":"2023-03-12T15:43:48.464495Z","shell.execute_reply":"2023-03-12T15:43:48.475159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_samples()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:48.478095Z","iopub.execute_input":"2023-03-12T15:43:48.479377Z","iopub.status.idle":"2023-03-12T15:43:49.828492Z","shell.execute_reply.started":"2023-03-12T15:43:48.479311Z","shell.execute_reply":"2023-03-12T15:43:49.827644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that the images are indeed enhanced, with the <font color=\"#0A9396\"><b><i>Glaucoma</i></b></font> instances also rotated and flipped, as intented in our pipeline.","metadata":{}},{"cell_type":"markdown","source":"## <font color=#264653><u>Model</u></font>\nWe will use <b><u>Transfer Learning</u></b>, as a commonly used approach for dealing with insufficient labeled data. We will use a <code>EfficientNetB0</code> base, pretrained on ImageNet, and will fine-tune it.\n\n<figure>\n<img src=\"https://1.bp.blogspot.com/-DjZT_TLYZok/XO3BYqpxCJI/AAAAAAAAEKM/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs/s1600/image2.png\" alt=\"EfficientNet-B0 anatomy\" width=\"600\">\n\n<figurecaption><i>EfficientNet-B0 architecture, where MBConv is a mobile inverted bottleneck convolution layer <sup>[[6]](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)</sup>.</i></figurecaption>\n</figure>\n\nWe will freeze the parameters of all layers in the pretrained base, barring the final block (excluding the classifying head as well, of course).","metadata":{}},{"cell_type":"code","source":"def plot_train(history, start_epoch=0, is_shift_val=True, suptitle=None, subtitle=None, **fig_opts):\n    history_df = pd.DataFrame(history.history)\n    mets = history_df.columns[history_df.columns.str.startswith(\"val\")].str.replace(\"val_\",\"\").tolist()\n    L = history_df.shape[0]\n    \n    nrows, ncols = get_subplot_dims(len(mets))\n\n    fig = plt.figure(**fig_opts)\n    fig.subplots_adjust(hspace=0.75, wspace=0.75)\n    fig.patch.set_facecolor(COLORS[\"fig_bg\"])\n    \n    for i,met in enumerate(mets):\n        if met==\"loss\":\n            value_train = np.round(history_df[met].min(),4)\n            value_val = np.round(history_df[\"val_\"+met].min(),4)\n        else:\n            value_train = np.round(history_df[met].max(),4)\n            value_val = np.round(history_df[\"val_\"+met].max(),4)\n            \n        ax = plt.subplot(nrows, ncols, i+1)\n        \n        ax.plot(np.arange(start_epoch, L), history_df[met].iloc[start_epoch:], color=COLORS[\"split\"][\"train\"])\n        ax.plot(np.arange(start_epoch - is_shift_val*0.5, L - is_shift_val*0.5), history_df[\"val_\"+met].iloc[start_epoch:], color=COLORS[\"split\"][\"val\"])\n        \n        ax.set_title(met, **FONT_KW[\"subplot_title_small\"], pad=30)\n        ax.set_xlabel(\"epoch\", **FONT_KW[\"plot_text_small\"])\n        ax.set_ylabel(\"\")\n        ax.legend([\"train\", \"validation\"], prop={\"family\": \"serif\", \"size\": 8}, facecolor=COLORS[\"fig_bg\"])\n            \n        plt.xticks(**FONT_KW[\"plot_text_small\"])\n        plt.yticks(**FONT_KW[\"plot_text_small\"])\n        \n        ax.text(\n            0.5, 1.1,\n            f\"Best train {met}: {value_train}\\n\" +\\\n            f\"Best val {met}: {value_val}\",\n            **FONT_KW[\"plot_text_small\"],\n            color=COLORS[\"plot_text\"],\n            transform=ax.transAxes,\n            ha=\"center\",\n            bbox={\n                \"boxstyle\": \"Round\",\n                \"fill\": False,\n                \"edgecolor\": COLORS[\"plot_text\"]\n            }\n        )\n        \n        ax.set_facecolor(COLORS[\"fig_bg\"])\n        for spine in [\"right\", \"top\"]:\n            ax.spines[spine].set_visible(False)\n\n    if suptitle is not None:\n        plt.suptitle(\n            x=0.5, y=1.005,\n            t=suptitle,\n            ha=\"center\",\n            va=\"bottom\",\n            **FONT_KW[\"plot_title_small\"],\n            color=COLORS[\"plot_text\"]\n        )\n        \n    if subtitle is not None:\n        plt.figtext(\n            x=0.5, y=0.98,\n            s=subtitle,\n            ha=\"center\",\n            va=\"bottom\",\n            **FONT_KW[\"plot_subtitle\"],\n            color=COLORS[\"plot_text\"]\n        )\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:49.830694Z","iopub.execute_input":"2023-03-12T15:43:49.83106Z","iopub.status.idle":"2023-03-12T15:43:49.852168Z","shell.execute_reply.started":"2023-03-12T15:43:49.831012Z","shell.execute_reply":"2023-03-12T15:43:49.851074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also calculate the initial bias, based on the class distribution in the training data, to allow the model's final layer have a better starting point.","metadata":{}},{"cell_type":"code","source":"label_count = np.bincount(y_train.argmax(axis=1))\ntot = np.sum(label_count)\ninitial_bias = [np.log(c/tot) for c in label_count]\ninitial_bias","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:49.854743Z","iopub.execute_input":"2023-03-12T15:43:49.855089Z","iopub.status.idle":"2023-03-12T15:43:49.866916Z","shell.execute_reply.started":"2023-03-12T15:43:49.855057Z","shell.execute_reply":"2023-03-12T15:43:49.865751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And also the class weights (note that these were not used in training, as experimenting concluded they do not inprove the performance in this case).","metadata":{}},{"cell_type":"code","source":"class_weight = {c: (1/lc)*(tot/NUM_CLASSES) for c,lc in enumerate(label_count)}\nclass_weight","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:49.868877Z","iopub.execute_input":"2023-03-12T15:43:49.869262Z","iopub.status.idle":"2023-03-12T15:43:49.876955Z","shell.execute_reply.started":"2023-03-12T15:43:49.869226Z","shell.execute_reply":"2023-03-12T15:43:49.876005Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"effnet_model = MobileNetV3Small(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\")\n\nlast_layer_include = 5\n\nif last_layer_include is not None:\n    for layer in effnet_model.layers[:-last_layer_include]:\n        layer.trainable=False\nelse:\n    for layer in effnet_model.layers:\n        layer.trainable=False\n\npreprocess_input_effnet = tf.keras.applications.mobilenet_v3.preprocess_input","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:49.878409Z","iopub.execute_input":"2023-03-12T15:43:49.879568Z","iopub.status.idle":"2023-03-12T15:43:51.246826Z","shell.execute_reply.started":"2023-03-12T15:43:49.879474Z","shell.execute_reply":"2023-03-12T15:43:51.245766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The final model will contain the pretrained base, along with its preprocessing unit (in the case of <code>EfficientNet-B0</code>, this is a passthrough function, since for EfficientNet, input preprocessing is included as part of the model). We then add a <b><i>dropout</i></b> layer, followed by <b><i>global averaging</i></b> and <b><i>batch normalization</i></b>. The kernel of the final layer will be regularized by $\\ell_1$.","metadata":{}},{"cell_type":"code","source":"def build_model(base, preprocess_input, output_bias=None, dropout=0.0, L1=1e-3, name=\"model\"):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(*IMAGE_SIZE, 3)),\n        \n        tf.keras.layers.Lambda(preprocess_input, name='preprocessing'),\n        base,\n        \n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.GlobalAveragePooling2D(),       \n        tf.keras.layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Dense(\n            NUM_CLASSES,\n            activation='softmax',\n            bias_initializer=output_bias,\n            kernel_regularizer=tf.keras.regularizers.l1(L1)\n        )\n    ], name=name)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.248275Z","iopub.execute_input":"2023-03-12T15:43:51.248684Z","iopub.status.idle":"2023-03-12T15:43:51.257756Z","shell.execute_reply.started":"2023-03-12T15:43:51.248646Z","shell.execute_reply":"2023-03-12T15:43:51.255915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will train the model with a <b><i>Focal Loss</i></b> function, as it was previously shown to improve performance when dealing with class imbalance <sup>[[7]](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1708.02002.pdf)</sup>. By using focal loss, the model is incentivized to focus on learning the samples it still has difficulty classifying.\n\nWe will also monitor several metrics along with the loss function: AUC ROC, accuracy, F1 score, and the AUC PR (area under the percision-recall curve). These metrics will assist in better evaluating the model's performance, given the class imbalance prominent in the data.","metadata":{}},{"cell_type":"code","source":"learning_rate = 3e-4\ngamma = 2\ndropout = 0.1\nL1 = 2e-3\n\nwith strategy.scope():\n    model = build_model(effnet_model, preprocess_input_effnet, name=\"effnet_model\", output_bias=initial_bias, dropout=dropout, L1=L1)\n\n    METRICS = [\n        tf.keras.metrics.AUC(name=\"auc\"),\n        tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n        tfa.metrics.F1Score(num_classes=NUM_CLASSES, average=\"weighted\", name=\"f1\"),\n        tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n    ]\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma),\n        metrics=METRICS\n    )","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.259573Z","iopub.execute_input":"2023-03-12T15:43:51.260361Z","iopub.status.idle":"2023-03-12T15:43:51.738956Z","shell.execute_reply.started":"2023-03-12T15:43:51.260238Z","shell.execute_reply":"2023-03-12T15:43:51.737984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.740438Z","iopub.execute_input":"2023-03-12T15:43:51.740796Z","iopub.status.idle":"2023-03-12T15:43:51.778858Z","shell.execute_reply.started":"2023-03-12T15:43:51.740761Z","shell.execute_reply":"2023-03-12T15:43:51.778097Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will be using a larger than usuall <b><i>batch size</i></b>, to make sure each batch has a higher probability of containing at least some minority class instances.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 200\nBATCH_SIZE = 128","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.780157Z","iopub.execute_input":"2023-03-12T15:43:51.780515Z","iopub.status.idle":"2023-03-12T15:43:51.786872Z","shell.execute_reply.started":"2023-03-12T15:43:51.780478Z","shell.execute_reply":"2023-03-12T15:43:51.78592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will also use a learning rate scheduler (reducing the learning rate on plateu), and employ early stopping.","metadata":{}},{"cell_type":"code","source":"# reduce learning rate on plateu\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    factor=0.75,\n    patience=10,\n    verbose=1,\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=1e-6,\n)\n\n# early stopping\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    patience=EPOCHS//10,\n    restore_best_weights=True,\n    verbose=1,\n)\n\n# checkpoint\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"/kaggle/working/ocir_model_initial.h5\", save_best_only=True)\n\ncallbacks = [checkpoint_cb, early_stopping_cb, reduce_lr]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.788368Z","iopub.execute_input":"2023-03-12T15:43:51.788767Z","iopub.status.idle":"2023-03-12T15:43:51.799372Z","shell.execute_reply.started":"2023-03-12T15:43:51.788731Z","shell.execute_reply":"2023-03-12T15:43:51.798305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, y_val),\n    verbose=1,\n    callbacks=callbacks,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:43:51.800786Z","iopub.execute_input":"2023-03-12T15:43:51.801205Z","iopub.status.idle":"2023-03-12T15:49:04.807437Z","shell.execute_reply.started":"2023-03-12T15:43:51.801167Z","shell.execute_reply":"2023-03-12T15:49:04.806505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_train(history, start_epoch=1, figsize=(10,8), suptitle=\"EfficientNet Model\", subtitle=f\"Dropout={dropout}, L1={L1}, $\\gamma$={gamma}, Batch={BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:04.813513Z","iopub.execute_input":"2023-03-12T15:49:04.813825Z","iopub.status.idle":"2023-03-12T15:49:05.565298Z","shell.execute_reply.started":"2023-03-12T15:49:04.813798Z","shell.execute_reply":"2023-03-12T15:49:05.564159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font color=#264653><u>Model Evaluation</u></font>\nWe will examine the model's performance on the validation set, and later on the test set:","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, figsize=(16,6), cmap=\"Blues\", suptitle=None):\n    cm = confusion_matrix(y_true, y_pred)\n    cm_norm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n    \n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n    fig.patch.set_facecolor(COLORS[\"fig_bg\"])\n\n    sns.heatmap(\n        cm,\n        annot=True,\n        annot_kws=FONT_KW[\"plot_text\"],\n        fmt=\"d\",\n        linewidths=3.0,\n        linecolor=COLORS[\"fig_bg\"],\n        cmap=cmap,\n        cbar=False,\n        square=True,\n        xticklabels=[class_short2full[k] for k in class_dict.keys()],\n        yticklabels=[class_short2full[k] for k in class_dict.keys()],\n        ax=ax1\n    )\n\n    ax1.set_title(\"Confusion Matrix\", **FONT_KW[\"subplot_title_small\"])\n    ax1.set_xlabel(\"Predicted Labels\", **FONT_KW[\"plot_label_small\"])\n    ax1.set_ylabel(\"True Labels\", **FONT_KW[\"plot_label_small\"])\n    \n    ax1.set_facecolor(COLORS[\"fig_bg\"])\n    ax1.tick_params(axis=\"both\", length=0)\n    ax1.set_yticks(\n        np.arange(len(ax1.get_yticklabels()))+0.5,\n        [label.get_text() for label in ax1.get_yticklabels()],\n        **FONT_KW[\"plot_label_small\"]\n    )\n    ax1.set_xticks(\n        np.arange(len(ax1.get_xticklabels()))+0.5,\n        [label.get_text() for label in ax1.get_xticklabels()],\n        **FONT_KW[\"plot_label_small\"]\n    )\n    \n    for class_, i in class_dict.items():\n        ax1.get_xticklabels()[i].set_color(COLORS[\"class\"][class_])\n        ax1.get_yticklabels()[i].set_color(COLORS[\"class\"][class_])\n\n\n    sns.heatmap(\n        cm_norm,\n        annot=True,\n        annot_kws=FONT_KW[\"plot_text\"],\n        fmt= \".0%\" if np.all(np.allclose(cm_norm, cm_norm.astype(int))) else \".1%\",\n        linewidths=3.0,\n        linecolor=COLORS[\"fig_bg\"],\n        cmap=cmap,\n        cbar=False,\n        square=True,\n        xticklabels=[class_short2full[k] for k in class_dict.keys()],\n        yticklabels=[class_short2full[k] for k in class_dict.keys()],\n        ax=ax2\n    )\n\n    ax2.set_title(\"Confusion Matrix (Normalized)\", **FONT_KW[\"subplot_title_small\"])\n    ax2.set_xlabel(\"Predicted Labels\", **FONT_KW[\"plot_label_small\"])\n    ax2.set_ylabel(\"True Labels\", **FONT_KW[\"plot_label_small\"])\n    \n    ax2.set_facecolor(COLORS[\"fig_bg\"])\n    ax2.tick_params(axis=\"both\", length=0)\n    ax2.set_yticks(\n        np.arange(len(ax2.get_yticklabels()))+0.5,\n        [label.get_text() for label in ax2.get_yticklabels()],\n        **FONT_KW[\"plot_label_small\"]\n    )\n    ax2.set_xticks(\n        np.arange(len(ax2.get_xticklabels()))+0.5,\n        [label.get_text() for label in ax2.get_xticklabels()],\n        **FONT_KW[\"plot_label_small\"]\n    )\n        \n    for class_, i in class_dict.items():\n        ax2.get_xticklabels()[i].set_color(COLORS[\"class\"][class_])\n        ax2.get_yticklabels()[i].set_color(COLORS[\"class\"][class_])\n    \n    if suptitle is not None:\n        plt.suptitle(suptitle, y=0.98, **FONT_KW[\"plot_title_small\"])\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:05.568706Z","iopub.execute_input":"2023-03-12T15:49:05.569124Z","iopub.status.idle":"2023-03-12T15:49:05.587686Z","shell.execute_reply.started":"2023-03-12T15:49:05.569086Z","shell.execute_reply":"2023-03-12T15:49:05.58645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_val_pred = np.argmax(model.predict(X_val), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:05.589582Z","iopub.execute_input":"2023-03-12T15:49:05.589981Z","iopub.status.idle":"2023-03-12T15:49:07.353607Z","shell.execute_reply.started":"2023-03-12T15:49:05.589933Z","shell.execute_reply":"2023-03-12T15:49:07.352531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(\n    np.argmax(y_val, axis=1), y_val_pred,\n    figsize=(8,4),\n    cmap=COLORS[\"cmap_pos\"],\n    suptitle=\"Model Performance (Validation)\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:07.355283Z","iopub.execute_input":"2023-03-12T15:49:07.35566Z","iopub.status.idle":"2023-03-12T15:49:07.648878Z","shell.execute_reply.started":"2023-03-12T15:49:07.355624Z","shell.execute_reply":"2023-03-12T15:49:07.647951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test_prob = model.predict(X_test)\ny_test_pred = np.argmax(y_test_prob, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:07.650294Z","iopub.execute_input":"2023-03-12T15:49:07.651315Z","iopub.status.idle":"2023-03-12T15:49:08.582942Z","shell.execute_reply.started":"2023-03-12T15:49:07.651275Z","shell.execute_reply":"2023-03-12T15:49:08.581887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(\n    np.argmax(y_test, axis=1), y_test_pred,\n    figsize=(8,4),\n    cmap=COLORS[\"cmap_pos\"],\n    suptitle=\"Model Performance (Test)\"\n)\n\nreport = classification_report(np.argmax(y_test, axis=1), y_test_pred, target_names=[class_short2full[k] for k in class_dict.keys()])\ntest_roc_auc = roc_auc_score(np.argmax(y_test, axis=1), y_test_prob[:, 1])\n\nprint(report)\nprint(f\"     roc auc       {np.round(test_roc_auc, 2)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:49:08.584399Z","iopub.execute_input":"2023-03-12T15:49:08.584889Z","iopub.status.idle":"2023-03-12T15:49:08.88537Z","shell.execute_reply.started":"2023-03-12T15:49:08.584851Z","shell.execute_reply":"2023-03-12T15:49:08.884127Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tflite_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()\nwith open('/kaggle/working/ocular.tflite', 'wb') as f:\n  f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:56:17.218584Z","iopub.execute_input":"2023-03-12T15:56:17.218951Z","iopub.status.idle":"2023-03-12T15:56:51.078408Z","shell.execute_reply.started":"2023-03-12T15:56:17.218919Z","shell.execute_reply":"2023-03-12T15:56:51.077372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.system(\"rm -rf /kaggle/working/*\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T15:56:01.717788Z","iopub.execute_input":"2023-03-12T15:56:01.718273Z","iopub.status.idle":"2023-03-12T15:56:01.741223Z","shell.execute_reply.started":"2023-03-12T15:56:01.718233Z","shell.execute_reply":"2023-03-12T15:56:01.740332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}